{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# EEG-to-Text HMM Pipeline - Google Colab\n",
                "\n",
                "This notebook runs the complete EEG-to-text pipeline on Google Colab with GPU support.\n",
                "\n",
                "## Setup Instructions\n",
                "\n",
                "1. Upload your dataset to Google Drive\n",
                "2. Run the cells in order\n",
                "3. Training will take ~30-45 minutes\n",
                "\n",
                "**Expected Results:**\n",
                "- ~344 unique sentences\n",
                "- ~20-40% accuracy\n",
                "- Models saved to Google Drive"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Mount Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages (PyTorch is pre-installed on Colab)\n",
                "!pip install -q pandas numpy\n",
                "\n",
                "# Check GPU availability\n",
                "import torch\n",
                "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Set Up Project Directory\n",
                "\n",
                "**IMPORTANT:** Update the `DRIVE_PATH` below to point to your uploaded dataset folder in Google Drive."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# UPDATE THIS PATH to where you uploaded your data in Google Drive\n",
                "DRIVE_PATH = '/content/drive/MyDrive/ML Project Data'\n",
                "\n",
                "# Change to project directory\n",
                "os.chdir(DRIVE_PATH)\n",
                "\n",
                "# Verify the directory structure\n",
                "print(\"Project directory contents:\")\n",
                "!ls -la\n",
                "\n",
                "print(\"\\nProcessed data directory:\")\n",
                "!ls processed_data/*.csv | head -5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Update Config for GPU\n",
                "\n",
                "This cell modifies the config to use GPU if available."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Update config to use GPU\n",
                "config_updates = '''\n",
                "# Update CNN_DEVICE to use GPU if available\n",
                "import torch\n",
                "CNN_DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"Using device: {CNN_DEVICE}\")\n",
                "'''\n",
                "\n",
                "# Append to config file\n",
                "with open('src/config.py', 'a') as f:\n",
                "    f.write('\\n' + config_updates)\n",
                "\n",
                "print(\"✓ Config updated for GPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Check Data Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Count data files\n",
                "import glob\n",
                "\n",
                "csv_files = glob.glob('processed_data/rawdata_*.csv')\n",
                "print(f\"Total CSV files: {len(csv_files)}\")\n",
                "\n",
                "# Check mapping file\n",
                "import pandas as pd\n",
                "mapping = pd.read_csv('processed_data/sentence_mapping.csv')\n",
                "print(f\"Mapping entries: {len(mapping)}\")\n",
                "print(f\"Unique sentences: {mapping['Content'].nunique()}\")\n",
                "\n",
                "# Show sample\n",
                "print(\"\\nSample entries:\")\n",
                "print(mapping.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Run Training Pipeline\n",
                "\n",
                "This will run the full training with:\n",
                "- All 5,915 files\n",
                "- 2x augmentation\n",
                "- 3 CNN epochs\n",
                "- GPU acceleration\n",
                "\n",
                "**Expected time:** 30-45 minutes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run the training pipeline with 2x augmentation\n",
                "!python main.py --num-aug 2 --save-models"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Alternative: Memory-Efficient Version\n",
                "\n",
                "If the above fails due to memory, use this instead:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run memory-efficient version\n",
                "!python main_memory_efficient.py --num-aug 2 --save-models"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Check Saved Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List saved models\n",
                "print(\"Saved models:\")\n",
                "!ls -lh checkpoints/\n",
                "\n",
                "# Check model sizes\n",
                "import os\n",
                "if os.path.exists('checkpoints/cnn_encoder.pth'):\n",
                "    size = os.path.getsize('checkpoints/cnn_encoder.pth') / 1e6\n",
                "    print(f\"\\nCNN Encoder: {size:.2f} MB\")\n",
                "\n",
                "if os.path.exists('checkpoints/hmm_models.pkl'):\n",
                "    size = os.path.getsize('checkpoints/hmm_models.pkl') / 1e6\n",
                "    print(f\"HMM Models: {size:.2f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Download Models (Optional)\n",
                "\n",
                "Download the trained models to your local machine."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "# Download CNN encoder\n",
                "if os.path.exists('checkpoints/cnn_encoder.pth'):\n",
                "    files.download('checkpoints/cnn_encoder.pth')\n",
                "    print(\"✓ Downloaded CNN encoder\")\n",
                "\n",
                "# Download HMM models\n",
                "if os.path.exists('checkpoints/hmm_models.pkl'):\n",
                "    files.download('checkpoints/hmm_models.pkl')\n",
                "    print(\"✓ Downloaded HMM models\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Test Inference (Optional)\n",
                "\n",
                "Load the trained models and test on a sample."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('src')\n",
                "\n",
                "import torch\n",
                "import numpy as np\n",
                "from feature_extractor import CNNEEGEncoder\n",
                "from predictor import SentencePredictor\n",
                "from data_loader import DataLoader\n",
                "\n",
                "# Load models\n",
                "print(\"Loading models...\")\n",
                "encoder = CNNEEGEncoder(input_channels=105, hidden_channels=32, sequence_length=5500)\n",
                "checkpoint = torch.load('checkpoints/cnn_encoder.pth', map_location='cpu')\n",
                "encoder.load_state_dict(checkpoint['model_state_dict'])\n",
                "encoder.eval()\n",
                "\n",
                "predictor = SentencePredictor(n_states=3, n_features=32)\n",
                "predictor.load('checkpoints/hmm_models.pkl')\n",
                "\n",
                "print(f\"✓ Loaded {len(predictor.models)} sentence models\")\n",
                "\n",
                "# Test on a random file\n",
                "loader = DataLoader('processed_data')\n",
                "loader.load_mapping()\n",
                "test_file = loader.get_all_files()[0]\n",
                "test_data = loader.load_padded_data(test_file, target_length=5500)\n",
                "true_text = loader.get_text_for_file(test_file)\n",
                "\n",
                "# Extract features and predict\n",
                "with torch.no_grad():\n",
                "    X_tensor = torch.tensor(test_data[np.newaxis, :, :], dtype=torch.float32)\n",
                "    features = encoder.get_features(X_tensor)\n",
                "    features_np = features.numpy()[0].T\n",
                "\n",
                "pred_text, score = predictor.predict(features_np)\n",
                "\n",
                "print(f\"\\nTest Prediction:\")\n",
                "print(f\"True: {true_text[:80]}...\")\n",
                "print(f\"Pred: {pred_text[:80]}...\")\n",
                "print(f\"Score: {score:.2f}\")\n",
                "print(f\"Match: {pred_text == true_text}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Notes\n",
                "\n",
                "### Memory Usage\n",
                "- Colab provides ~12-15 GB RAM\n",
                "- Full dataset with 2x augmentation needs ~12-15 GB\n",
                "- If you get memory errors, use `main_memory_efficient.py` instead\n",
                "\n",
                "### GPU Acceleration\n",
                "- CNN training will be much faster on GPU (~5-10x speedup)\n",
                "- HMM training runs on CPU (no GPU implementation)\n",
                "\n",
                "### Runtime Limits\n",
                "- Free Colab sessions timeout after 12 hours of inactivity\n",
                "- Training should complete in 30-45 minutes\n",
                "- Models are saved to your Google Drive\n",
                "\n",
                "### Troubleshooting\n",
                "- **Out of memory**: Use `main_memory_efficient.py` or reduce `--num-aug`\n",
                "- **Session timeout**: Models are saved incrementally, you can resume\n",
                "- **Slow training**: Make sure GPU is enabled (Runtime > Change runtime type > GPU)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}