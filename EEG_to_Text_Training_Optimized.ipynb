{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† EEG-to-Text Training Pipeline (Optimized)\n",
        "\n",
        "## üìã Overview\n",
        "- **Model**: Supervised CNN + HMM\n",
        "- **Expected Accuracy**: 20-40% (with ~100-150 classes)\n",
        "- **Training Time**: 2-4 hours\n",
        "- **Memory**: RAM-safe for Colab (5-6 GB peak)\n",
        "\n",
        "## ‚úÖ Key Optimizations Applied\n",
        "- Fixed augmentation bug (was returning wrong number of samples)\n",
        "- Reduced classes to most common sentences (better accuracy)\n",
        "- Optimized chunk size for Colab RAM limits\n",
        "- Fixed learning rate scheduler\n",
        "- 6x data augmentation\n",
        "\n",
        "## üìù Before You Start\n",
        "**You need to upload your dataset to Google Drive first!**\n",
        "\n",
        "1. Create a folder in Google Drive: `ML_Project_Dataset`\n",
        "2. Upload your dataset files there (~5,915 CSV files)\n",
        "3. Make sure you have:\n",
        "   - `rawdata_*.csv` files (5,915 files)\n",
        "   - `sentence_mapping.csv` file\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"\\n‚úÖ Google Drive mounted successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Clone Repository from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Clone repository (or pull latest changes if already exists)\n",
        "if not os.path.exists('/content/ML-Project-Data'):\n",
        "    print(\"üì• Cloning repository from GitHub...\")\n",
        "    !git clone https://github.com/Tejas-Chakkarwar/ML-Project-Data.git\n",
        "    print(\"‚úÖ Repository cloned!\")\n",
        "else:\n",
        "    print(\"‚úÖ Repository already exists\")\n",
        "    print(\"üì• Pulling latest changes...\")\n",
        "    !cd /content/ML-Project-Data && git pull origin main\n",
        "\n",
        "# Change to project directory\n",
        "os.chdir('/content/ML-Project-Data')\n",
        "print(f\"\\n‚úÖ Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Verify files\n",
        "print(\"\\nüìã Verifying code files:\")\n",
        "print(f\"  main_streaming_supervised.py: {'‚úÖ' if os.path.exists('main_streaming_supervised.py') else '‚ùå MISSING'}\")\n",
        "print(f\"  src/ directory: {'‚úÖ' if os.path.exists('src') else '‚ùå MISSING'}\")\n",
        "print(f\"  src/config.py: {'‚úÖ' if os.path.exists('src/config.py') else '‚ùå MISSING'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q torch numpy pandas scikit-learn\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "\n",
        "print(\"\\nüìä System Information:\")\n",
        "print(f\"  GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(\"\\n  ‚úÖ GPU enabled - Training will use GPU!\")\n",
        "else:\n",
        "    print(\"\\n  ‚ö†Ô∏è No GPU detected!\")\n",
        "    print(\"  Go to: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "\n",
        "print(\"\\n‚úÖ Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Copy Dataset from Google Drive to Local Storage\n",
        "\n",
        "‚ö†Ô∏è **IMPORTANT**: Update the `SOURCE` path below to match where you uploaded your dataset!\n",
        "\n",
        "Common paths:\n",
        "- `/content/drive/MyDrive/ML_Project_Dataset`\n",
        "- `/content/drive/MyDrive/Colab Notebooks/dataset`\n",
        "- `/content/drive/MyDrive/dataset`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"COPYING DATASET FROM GOOGLE DRIVE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ‚ö†Ô∏è UPDATE THIS PATH TO YOUR ACTUAL DATASET LOCATION!\n",
        "SOURCE = '/content/drive/MyDrive/ML_Project_Dataset'  # ‚Üê Change this!\n",
        "DEST = '/content/ML-Project-Data/processed_data'\n",
        "\n",
        "# Verify source exists\n",
        "if not os.path.exists(SOURCE):\n",
        "    print(f\"\\n‚ùå ERROR: Source path not found!\")\n",
        "    print(f\"   Path: {SOURCE}\")\n",
        "    print(\"\\nüìÅ Available folders in MyDrive:\")\n",
        "    for item in os.listdir('/content/drive/MyDrive'):\n",
        "        item_path = os.path.join('/content/drive/MyDrive', item)\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"   üìÅ {item}/\")\n",
        "    print(\"\\n‚ö†Ô∏è Please update the SOURCE path in the cell above!\")\n",
        "else:\n",
        "    # Create destination\n",
        "    os.makedirs(DEST, exist_ok=True)\n",
        "    \n",
        "    # Get list of files\n",
        "    all_files = sorted(os.listdir(SOURCE))\n",
        "    total_files = len(all_files)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Source found: {SOURCE}\")\n",
        "    print(f\"üìä Total files to copy: {total_files:,}\")\n",
        "    \n",
        "    # Check what's already copied\n",
        "    already_copied = set(os.listdir(DEST)) if os.path.exists(DEST) else set()\n",
        "    print(f\"üìä Already copied: {len(already_copied):,}\")\n",
        "    print(f\"üìä Remaining: {total_files - len(already_copied):,}\\n\")\n",
        "    \n",
        "    # Copy files with progress\n",
        "    copied = 0\n",
        "    failed = []\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for i, filename in enumerate(all_files, 1):\n",
        "        # Skip if already copied\n",
        "        if filename in already_copied:\n",
        "            continue\n",
        "        \n",
        "        src_path = os.path.join(SOURCE, filename)\n",
        "        dst_path = os.path.join(DEST, filename)\n",
        "        \n",
        "        # Try to copy with retry\n",
        "        max_retries = 3\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                if os.path.isfile(src_path):\n",
        "                    shutil.copy2(src_path, dst_path)\n",
        "                    copied += 1\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2)\n",
        "                else:\n",
        "                    failed.append(filename)\n",
        "        \n",
        "        # Progress update every 1000 files\n",
        "        if (i % 1000 == 0) or (i == total_files):\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"   [{i:,}/{total_files:,}] Progress ({elapsed/60:.1f} min, {len(failed)} failed)\")\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"COPY COMPLETE\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"‚è±Ô∏è  Time: {elapsed/60:.1f} minutes\")\n",
        "    print(f\"‚úÖ Copied: {copied:,} files\")\n",
        "    print(f\"üìä Already existed: {len(already_copied):,} files\")\n",
        "    print(f\"‚ùå Failed: {len(failed)} files\")\n",
        "    \n",
        "    # Verify\n",
        "    csv_files = glob.glob(f'{DEST}/rawdata_*.csv')\n",
        "    mapping_exists = os.path.exists(f'{DEST}/sentence_mapping.csv')\n",
        "    \n",
        "    print(f\"\\nüìã Final Verification:\")\n",
        "    print(f\"   CSV files: {len(csv_files):,}\")\n",
        "    print(f\"   Mapping file: {'‚úÖ' if mapping_exists else '‚ùå'}\")\n",
        "    \n",
        "    if len(csv_files) >= 5900 and mapping_exists:\n",
        "        print(\"\\nüéâ SUCCESS! Data is ready for training!\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  Warning: Only {len(csv_files):,} files (expected ~5,915)\")\n",
        "    \n",
        "    print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Verify Data and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content/ML-Project-Data')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"PRE-TRAINING VERIFICATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check data\n",
        "data_dir = 'processed_data'\n",
        "if os.path.exists(data_dir):\n",
        "    csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv') and f.startswith('rawdata')]\n",
        "    mapping_file = os.path.join(data_dir, 'sentence_mapping.csv')\n",
        "    \n",
        "    print(f\"\\n‚úÖ Data Status:\")\n",
        "    print(f\"   Directory: {data_dir}\")\n",
        "    print(f\"   CSV files: {len(csv_files):,}\")\n",
        "    print(f\"   Mapping file: {'‚úÖ' if os.path.exists(mapping_file) else '‚ùå'}\")\n",
        "    \n",
        "    if len(csv_files) >= 5900 and os.path.exists(mapping_file):\n",
        "        print(\"\\n   üéâ All data ready!\")\n",
        "    else:\n",
        "        print(f\"\\n   ‚ö†Ô∏è  Only {len(csv_files):,} files found (expected ~5,915)\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Data directory not found: {data_dir}\")\n",
        "    print(\"   Please run Step 4 to copy data first!\")\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "print(f\"\\n‚úÖ GPU Status:\")\n",
        "print(f\"   Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Check config\n",
        "print(f\"\\n‚úÖ Configuration:\")\n",
        "with open('src/config.py', 'r') as f:\n",
        "    for line in f:\n",
        "        if 'CNN_DEVICE' in line or 'MIN_SAMPLES_PER_SENTENCE' in line or 'NUM_AUGMENTATIONS' in line:\n",
        "            print(f\"   {line.strip()}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ Verification complete! Ready to train.\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Run Training (Optimized Settings)\n",
        "\n",
        "### üéØ Configuration:\n",
        "- **Classes**: ~100-150 (min_samples=25 for best quality)\n",
        "- **Augmentation**: 6x (more training data)\n",
        "- **Chunk size**: 400 (RAM-safe for Colab)\n",
        "- **Batch size**: 32 (optimized)\n",
        "- **Epochs**: 12 (good balance)\n",
        "\n",
        "### ‚è±Ô∏è Expected:\n",
        "- **Training time**: 2-4 hours\n",
        "- **Target accuracy**: 20-40%\n",
        "- **Memory usage**: ~5-6 GB RAM (safe for Colab)\n",
        "\n",
        "### üìä What to Watch:\n",
        "- Training accuracy should increase from ~1% ‚Üí 25-40%\n",
        "- Loss should decrease steadily\n",
        "- No \"Out of Memory\" errors\n",
        "\n",
        "---\n",
        "\n",
        "**‚ö†Ô∏è This cell will take 2-4 hours to complete. Don't close your browser!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "os.chdir('/content/ML-Project-Data')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ EEG-TO-TEXT TRAINING - OPTIMIZED\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüéØ Configuration:\")\n",
        "print(\"  ‚úÖ Min samples per class: 25 (~100-150 classes)\")\n",
        "print(\"  ‚úÖ Augmentation: 6x (more training data)\")\n",
        "print(\"  ‚úÖ Chunk size: 400 (RAM-safe)\")\n",
        "print(\"  ‚úÖ Batch size: 32\")\n",
        "print(\"  ‚úÖ Epochs: 12\")\n",
        "print(\"  ‚úÖ HMM states: 5\")\n",
        "print())\n",
        "print(\"üíæ Expected Memory: ~5-6 GB RAM (safe for Colab)\")\n",
        "print(\"‚è±Ô∏è  Expected Time: 2-4 hours\")\n",
        "print(\"üéØ Target Accuracy: 20-40%\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "!python main_streaming_supervised.py \\\n",
        "  --cnn-epochs 12 \\\n",
        "  --cnn-batch-size 32 \\\n",
        "  --hmm-states 5 \\\n",
        "  --num-aug 6 \\\n",
        "  --min-samples 25 \\\n",
        "  --chunk-size 400 \\\n",
        "  --save-models \\\n",
        "  --verbose\n",
        "\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"üéâ TRAINING COMPLETED IN {elapsed/60:.1f} MINUTES ({elapsed/3600:.1f} HOURS)\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Save Models to Google Drive\n",
        "\n",
        "Copy trained models to Google Drive so they persist after the session ends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create destination in Google Drive\n",
        "DRIVE_MODELS_DIR = '/content/drive/MyDrive/ML_Project_Models_Final'\n",
        "os.makedirs(DRIVE_MODELS_DIR, exist_ok=True)\n",
        "\n",
        "# Copy checkpoints\n",
        "LOCAL_CHECKPOINT_DIR = '/content/ML-Project-Data/checkpoints'\n",
        "\n",
        "if os.path.exists(LOCAL_CHECKPOINT_DIR):\n",
        "    print(\"üì¶ Saving models to Google Drive...\\n\")\n",
        "    \n",
        "    for filename in os.listdir(LOCAL_CHECKPOINT_DIR):\n",
        "        if filename.endswith(('.pth', '.pkl')):\n",
        "            src = os.path.join(LOCAL_CHECKPOINT_DIR, filename)\n",
        "            dst = os.path.join(DRIVE_MODELS_DIR, filename)\n",
        "            \n",
        "            shutil.copy2(src, dst)\n",
        "            size_mb = os.path.getsize(dst) / 1e6\n",
        "            print(f\"‚úÖ {filename} ({size_mb:.1f} MB)\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Models saved to: {DRIVE_MODELS_DIR}\")\n",
        "    print(\"   These will persist even after session ends!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No checkpoints found. Did training complete successfully?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8 (Optional): Download Models to Local Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "checkpoint_dir = '/content/ML-Project-Data/checkpoints'\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"üì• Downloading models...\\n\")\n",
        "    \n",
        "    for filename in os.listdir(checkpoint_dir):\n",
        "        if filename.endswith(('.pth', '.pkl')):\n",
        "            filepath = os.path.join(checkpoint_dir, filename)\n",
        "            print(f\"Downloading {filename}...\")\n",
        "            files.download(filepath)\n",
        "    \n",
        "    print(\"\\n‚úÖ Downloads started! Check your browser's download folder.\")\n",
        "else:\n",
        "    print(\"‚ùå No checkpoints to download.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Understanding Your Results\n",
        "\n",
        "### What's Good Performance?\n",
        "\n",
        "With **~100-150 classes**:\n",
        "- **Random guessing**: ~0.67-1.0%\n",
        "- **Poor**: 5-10%\n",
        "- **Decent**: 15-25%\n",
        "- **Good**: 25-35% ‚≠ê\n",
        "- **Excellent**: 35-45%\n",
        "\n",
        "### If Accuracy is Low (<15%):\n",
        "1. Reduce classes further: Try `--min-samples 40` (top 50-80 classes)\n",
        "2. Train longer: Try `--cnn-epochs 20`\n",
        "3. Increase augmentation: Try `--num-aug 10`\n",
        "\n",
        "### If You Get \"Out of Memory\":\n",
        "1. Reduce chunk size: Try `--chunk-size 300`\n",
        "2. Reduce batch size: Try `--cnn-batch-size 16`\n",
        "3. Reduce augmentation: Try `--num-aug 4`\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ Training Complete!\n",
        "\n",
        "**Your trained models are saved in:**\n",
        "- Google Drive: `/content/drive/MyDrive/ML_Project_Models_Final/`\n",
        "- Local (temporary): `/content/ML-Project-Data/checkpoints/`\n",
        "\n",
        "**Files:**\n",
        "- `cnn_encoder.pth` - Trained CNN feature extractor\n",
        "- `hmm_models.pkl` - Trained HMM models for each sentence\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
