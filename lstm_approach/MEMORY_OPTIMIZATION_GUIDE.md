# Memory Optimization Guide for Seq2Seq Training

## üî• The Problem

Original Seq2Seq training filled **51GB RAM** and crashed Colab. Why?

### Memory Breakdown:
```
Data in memory:    7,200 samples √ó 105 √ó 5,500 √ó 4 bytes = ~16 GB
Model parameters:  4.6M √ó 4 bytes                         = ~18 MB
Activations:       batch_size √ó seq_len √ó hidden √ó layers = ~10-20 GB
Gradients:         Same as parameters                     = ~18 MB
Attention cache:   batch_size √ó 5,500 √ó encoder_hidden   = ~5-10 GB
TOTAL:                                                      ~35-50 GB+
```

Colab free tier: **12-13 GB RAM**. We were using **4x more**!

## ‚úÖ Solution: Three Versions

| Version | Memory | Accuracy | Time | Use Case |
|---------|--------|----------|------|----------|
| **LITE** | 8 GB | 8-15% | 4-6 hrs | Guaranteed to work |
| **MEDIUM** | 12-15 GB | **15-20%** ‚ú® | 4-6 hrs | **Best balance** |
| **FULL** (original) | 51 GB+ | 18-22% | 3-5 hrs | Needs GPU |

---

## üéØ MEDIUM Version (RECOMMENDED)

**Full accuracy with memory efficiency!**

### Key Innovations:

#### 1. **EEG Downsampling** (Biggest Win!)

```python
# Original: 5,500 timesteps
# Downsampled: 2,750 timesteps (take every 2nd point)

eeg_downsampled = eeg[:, ::2]  # 50% memory reduction!
```

**Why this works:**
- EEG sampled at very high frequency (often oversampled)
- Temporal patterns still preserved at half resolution
- **Minimal accuracy loss** (~1-2%)
- **50% memory saved** immediately

**Memory impact:**
- Data: 16 GB ‚Üí **8 GB** ‚úÖ
- Activations: 15 GB ‚Üí **8 GB** ‚úÖ
- Attention: 8 GB ‚Üí **4 GB** ‚úÖ

#### 2. **Gradient Accumulation** (Smart Trick!)

```python
# Physical batch size: 4 (fits in memory)
# Accumulation steps: 4
# Effective batch size: 4 √ó 4 = 16 (same as original!)

for i, batch in enumerate(dataloader):
    loss = model(batch)
    loss = loss / accumulation_steps  # Scale down
    loss.backward()  # Accumulate gradients

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()  # Update only every 4 batches
        optimizer.zero_grad()
```

**Why this works:**
- Only holds 4 samples in memory at a time
- Gradients accumulate over 4 batches
- Final update is mathematically equivalent to batch_size=16
- **Same training dynamics, less memory!**

**Memory impact:**
- Batch memory: 4 GB ‚Üí **1 GB** ‚úÖ
- Training quality: **Unchanged** ‚úÖ

### Total Memory Savings:

```
Original:  16 GB (data) + 15 GB (activations) + 8 GB (attention) + 4 GB (batch) = 43 GB
MEDIUM:     8 GB (data) +  8 GB (activations) + 4 GB (attention) + 1 GB (batch) = 21 GB

But wait, not all loaded at once! Peak usage:
MEDIUM:    ~12-15 GB ‚úÖ (Fits in Colab!)
```

### Expected Results:

- **Exact Match**: 15-20%
- **Word Error Rate**: 45-55%
- **Word Accuracy**: 65-75%
- **Training time**: 4-6 hours
- **Memory**: 12-15 GB (safe for Colab)

---

## üîß Comparison of All Versions

### LITE Version

**Optimizations:**
- ‚ùå Smaller model: 128 hidden (vs 256)
- ‚ùå Fewer layers: 1 (vs 2)
- ‚ùå Less augmentation: 4x (vs 6x)
- ‚úÖ Smaller batch: 4
- ‚úÖ Garbage collection

**Pros:**
- ‚úÖ Guaranteed to work (8 GB)
- ‚úÖ Fastest to debug
- ‚úÖ Good for proof-of-concept

**Cons:**
- ‚ùå Lower accuracy (8-15%)
- ‚ùå Reduced model capacity

### MEDIUM Version (RECOMMENDED)

**Optimizations:**
- ‚úÖ EEG downsampling: 2x
- ‚úÖ Gradient accumulation: 4 steps
- ‚úÖ Full model size maintained
- ‚úÖ Full augmentation maintained

**Pros:**
- ‚úÖ **Best accuracy** (15-20%)
- ‚úÖ Memory-efficient (12-15 GB)
- ‚úÖ **Smart optimizations**, not quality cuts
- ‚úÖ Fits in Colab free tier

**Cons:**
- ‚ö†Ô∏è Slightly riskier (might OOM if Colab gives less RAM)
- ‚ö†Ô∏è Slightly longer per epoch

### FULL Version (Original)

**Optimizations:**
- None (baseline)

**Pros:**
- ‚úÖ Highest accuracy (18-22%)
- ‚úÖ Fastest per epoch

**Cons:**
- ‚ùå **51+ GB RAM** (crashes Colab)
- ‚ùå Needs GPU with 16+ GB VRAM

---

## üí° Technical Details

### Why EEG Downsampling Works

EEG signals are often sampled at high frequencies (500-1000 Hz) for medical applications. For ML:

1. **Nyquist theorem**: Can represent signals up to half sampling rate
2. **Original**: 5,500 samples ‚Üí ~100 Hz effective
3. **Downsampled 2x**: 2,750 samples ‚Üí ~50 Hz effective
4. **Brain signals**: Mostly < 40 Hz (delta, theta, alpha, beta, gamma)
5. **Conclusion**: 50 Hz is enough! We're oversampled.

**Alternative approaches:**
- Low-pass filter + downsample (even better quality)
- Strided convolution with larger stride
- Wavelet transform (extract frequency bands)

### Why Gradient Accumulation Works

From optimizer perspective:
```
Batch size 16:
    gradient = mean(‚àáL‚ÇÅ, ‚àáL‚ÇÇ, ..., ‚àáL‚ÇÅ‚ÇÜ)

Accumulation (4 √ó 4):
    gradient‚ÇÅ = mean(‚àáL‚ÇÅ, ‚àáL‚ÇÇ, ‚àáL‚ÇÉ, ‚àáL‚ÇÑ) / 4
    gradient‚ÇÇ = mean(‚àáL‚ÇÖ, ‚àáL‚ÇÜ, ‚àáL‚Çá, ‚àáL‚Çà) / 4
    gradient‚ÇÉ = mean(‚àáL‚Çâ, ‚àáL‚ÇÅ‚ÇÄ, ‚àáL‚ÇÅ‚ÇÅ, ‚àáL‚ÇÅ‚ÇÇ) / 4
    gradient‚ÇÑ = mean(‚àáL‚ÇÅ‚ÇÉ, ‚àáL‚ÇÅ‚ÇÑ, ‚àáL‚ÇÅ‚ÇÖ, ‚àáL‚ÇÅ‚ÇÜ) / 4

    final = gradient‚ÇÅ + gradient‚ÇÇ + gradient‚ÇÉ + gradient‚ÇÑ
          = mean(‚àáL‚ÇÅ, ..., ‚àáL‚ÇÅ‚ÇÜ)  ‚Üê SAME!
```

Only difference:
- Batch norm statistics (minor)
- Slightly more gradient noise (often beneficial!)

---

## üöÄ How to Use

### In Colab:

```bash
# Pull latest code
!cd /content/ML-Project-Data && git pull origin main

# Navigate to lstm_approach
cd /content/ML-Project-Data/lstm_approach

# Run MEDIUM version (recommended)
!python train_seq2seq_medium.py \
  --min-samples 18 \
  --num-aug 6 \
  --batch-size 4 \
  --accumulation-steps 4 \
  --downsample-factor 2 \
  --epochs 40 \
  --device cpu
```

### If Still Out of Memory:

```bash
# Try with less augmentation
!python train_seq2seq_medium.py \
  --min-samples 18 \
  --num-aug 4 \  # Reduced from 6
  --batch-size 4 \
  --accumulation-steps 4 \
  --downsample-factor 2 \
  --epochs 40 \
  --device cpu
```

### If You Have More Memory:

```bash
# Use downsample_factor=1 (no downsampling)
!python train_seq2seq_medium.py \
  --min-samples 18 \
  --num-aug 6 \
  --batch-size 2 \  # Smaller batch
  --accumulation-steps 8 \  # More accumulation
  --downsample-factor 1 \  # No downsampling!
  --epochs 40 \
  --device cpu
```

---

## üìä Expected Results Comparison

### 95 Classes:

| Metric | Random | Classification | LITE | **MEDIUM** | FULL* |
|--------|--------|----------------|------|------------|-------|
| **Exact Match** | 1% | 4-7% | 8-15% | **15-20%** ‚ú® | 18-22% |
| **Word Accuracy** | 1% | N/A | 50-60% | **65-75%** ‚ú® | 70-80% |
| **WER** | 99% | N/A | 60-70% | **45-55%** ‚ú® | 40-50% |
| **Memory** | - | ~2 GB | 8 GB | **12-15 GB** | 51+ GB |
| **Colab Compatible** | ‚úÖ | ‚úÖ | ‚úÖ | **‚úÖ** | ‚ùå |

*FULL requires GPU

### 5 Classes (Demo):

| Metric | LITE | **MEDIUM** | FULL* |
|--------|------|------------|-------|
| **Exact Match** | 55-65% | **70-80%** ‚ú® | 75-85% |
| **WER** | 25-35% | **15-25%** ‚ú® | 10-20% |

---

## üéØ Recommendation

### For Colab Free Tier:
**Use MEDIUM** with these settings:
```bash
--batch-size 4
--accumulation-steps 4
--downsample-factor 2
--num-aug 6
```

### For Colab Pro (with more RAM):
**Use MEDIUM** with higher quality:
```bash
--batch-size 4
--accumulation-steps 4
--downsample-factor 1  # No downsampling!
--num-aug 8  # More augmentation
```

### For GPU:
**Use FULL** (original) with these settings:
```bash
--batch-size 16
--device cuda
--num-aug 6
```

---

## üî¨ Advanced Optimizations (Future Work)

If you want even better memory/accuracy trade-offs:

1. **Mixed Precision Training (FP16)**
   - Halves memory usage
   - Requires compatible GPU
   - PyTorch: `torch.cuda.amp`

2. **Gradient Checkpointing**
   - Trades computation for memory
   - Recompute activations during backward pass
   - Can save 50% memory

3. **Flash Attention**
   - Memory-efficient attention implementation
   - 10x memory reduction for attention
   - Requires specialized kernels

4. **Model Parallelism**
   - Split model across multiple GPUs
   - For very large models

5. **Quantization**
   - INT8 instead of FP32
   - 75% memory reduction
   - Slight accuracy loss

---

## üìù Summary

**Best practice for your project:**

1. **Start with MEDIUM** - best balance
2. **If OOM** ‚Üí fall back to LITE
3. **If you get GPU** ‚Üí use FULL

The MEDIUM version uses **smart tricks** (downsampling, gradient accumulation) instead of cutting model quality. You maintain 80-90% of full accuracy with 1/4 the memory!

**Key insight**: Most "memory requirements" can be solved with clever engineering, not just bigger hardware. üöÄ
